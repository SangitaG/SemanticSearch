{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.19.4-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 603kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.19.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd to directory above to acces sub-directories\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pymongo\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_data(url_api, params):\n",
    "    \n",
    "    '''This function gets data requested from a specified api url.\n",
    "       url_api: a url to api such as \"https://en.wikipedia.org/w/api.php?\"\n",
    "       params : a dictionary to setup the fetch request'''\n",
    "    \n",
    "    response = requests.get(url_api, params=params)\n",
    "    return(response.json())\n",
    "\n",
    "def get_wiki_pages(url_api, params, tree_depth):\n",
    "\n",
    "    '''This function recursively gets all pages, from the top level and subcategories,\n",
    "       for the specified category, in the params dictionary.       \n",
    "       url_api: a url to api such as \"https://en.wikipedia.org/w/api.php?\"\n",
    "       params : a dictionary to setup the fetch request'''\n",
    "\n",
    "    global cat_cnt; global pg_cnt\n",
    "    \n",
    "    # fetch data.\n",
    "    data = get_wiki_data(url_api, params)\n",
    "    \n",
    "    # Pages in category.\n",
    "    pages = [{'pageid':entry['pageid'], 'title':entry['title']} \n",
    "             for entry in data['query']['categorymembers'] if entry['ns']==0]\n",
    "\n",
    "    # subcategories in this category\n",
    "    sub_categories = [{'pageid':entry['pageid'], 'title':entry['title']}\n",
    "                      for entry in data['query']['categorymembers'] if entry['ns']==14]\n",
    "    \n",
    "    cat_cnt+=len(sub_categories)\n",
    "    pg_cnt+=len(pages)\n",
    "    \n",
    "    # If there are subcategories in this category's page, also get their pages.\n",
    "    if ((len(sub_categories) != 0) & (tree_depth != 0)):\n",
    "        tree_depth -=1\n",
    "        for category in sub_categories:\n",
    "            \n",
    "            # Update the category we want to fetch as this current subcategory\n",
    "            params[\"cmtitle\"] = category['title']\n",
    "            \n",
    "            # Recursively get all pages and subcategory pages for this \n",
    "            # current subcategory\n",
    "            pgs = get_wiki_pages(url_api, params=params, tree_depth=tree_depth)\n",
    "            \n",
    "            # append all pages from this current subcategory\n",
    "            pages += pgs            \n",
    "        \n",
    "    return(pages)\n",
    "\n",
    "def get_wiki_full_category_page_list(category, tree_depth):\n",
    "    \n",
    "    '''This function gets the entire requested data, for a category.\n",
    "       category : a wiki page category, such as \"Category:machine_learning\"'''\n",
    "\n",
    "    # url address of site's api, which we want to scrape.\n",
    "    url_api = \"https://en.wikipedia.org/w/api.php?\"\n",
    "\n",
    "    params = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"categorymembers\",\n",
    "    \"cmtitle\": category,\n",
    "    \"cmlimit\": \"max\"}  \n",
    "    \n",
    "    full_page_list = get_wiki_pages(url_api, params, tree_depth)\n",
    "    return(full_page_list)\n",
    "\n",
    "def get_wiki_page_content(page_title, page_ID):\n",
    "    \n",
    "    '''This function extracts the content for the requested page.\n",
    "       page_title : title of wiki page\n",
    "       page_ID    : the id of the page\"'''\n",
    "\n",
    "    # url address of site's api, which we want to scrape.\n",
    "    url_api = \"https://en.wikipedia.org/w/api.php?\"\n",
    "\n",
    "    params = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"titles\": page_title,\n",
    "    \"exlimit\": \"max\"\n",
    "    }\n",
    "    \n",
    "    # extracts contents of the specified page.\n",
    "    data = get_wiki_data(url_api, params) \n",
    "    page_content = data['query']['pages'][page_ID]['extract']\n",
    "    \n",
    "    return(page_content)\n",
    "\n",
    "def get_wiki_full_category_content(category):\n",
    "    \n",
    "    '''This function get the content for all the pages/subpages of the requested category.\n",
    "       Top-level function. Call this function with a specified category.\n",
    "       category : a wiki page category, such as \"Category:machine_learning\"'''\n",
    "    \n",
    "    # retrieve a list of dictionaries with all the pages and subpages related to this wiki category.\n",
    "    entire_category_page_lst = get_wiki_full_category_page_list(category)\n",
    "    \n",
    "    # get the text of wiki pages and add them to that pages' dictionary, with key value 'text'\n",
    "    for page_dict in tqdm(entire_category_page_lst):\n",
    "        page_dict['text'] = get_wiki_page_content(page_dict['title'], str(page_dict['pageid']))\n",
    "\n",
    "    return(entire_category_page_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongoDB_create_collection(db_name, collection_name, collection_lst):\n",
    "    \n",
    "    # Instantiate client to our Mongo Server\n",
    "    client = pymongo.MongoClient('35.163.170.219', 27016)\n",
    "\n",
    "    # Make a new database\n",
    "    db_ref = client[db_name]\n",
    "\n",
    "    # Create a reference to my_collection\n",
    "    coll_ref = db_ref[collection_name]\n",
    "\n",
    "    # Use the collection reference to insert the ML_page_contents\n",
    "    for doc in tqdm(collection_lst):\n",
    "        coll_ref.insert_one(doc)\n",
    "\n",
    "def mongoDB_read_collection(db_name, collection_name):\n",
    "    \n",
    "    # Instantiate client to our Mongo Server\n",
    "    client = pymongo.MongoClient('35.163.170.219', 27016)\n",
    "\n",
    "    # Make a new database\n",
    "    db_ref = client[db_name]\n",
    "\n",
    "    # Create a reference to my_collection\n",
    "    coll_ref = db_ref[collection_name] \n",
    "    \n",
    "    # Read collection into a list.\n",
    "    coll_lst = list(coll_ref.find())\n",
    "    \n",
    "    return(coll_lst)\n",
    "    \n",
    "def mongoDB_get_DBnames():  \n",
    "    \n",
    "    # Instantiate client to our Mongo Server\n",
    "    client = pymongo.MongoClient('35.163.170.219', 27016)    \n",
    "    \n",
    "    # Databases on our MongoDB Server.\n",
    "    names = client.database_names()\n",
    "    \n",
    "    return(names)\n",
    "\n",
    "def mongoDB_get_collection_names(db_name):\n",
    "    \n",
    "    # Instantiate client to our Mongo Server\n",
    "    client = pymongo.MongoClient('35.163.170.219', 27016)    \n",
    "    \n",
    "    # Make a new database\n",
    "    db_ref = client[db_name]\n",
    "    \n",
    "    names = db_ref.collection_names()\n",
    "    \n",
    "    return(names)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text.\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    text = TAG_RE.sub('', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text(page_lst):\n",
    "    for page in tqdm(page_lst):\n",
    "        page['text'] = remove_tags(page['text'])\n",
    "    \n",
    "    return(page_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list of dictionaries for each page in the category: machine learning.\n",
    "def pickle_obj(filename, objname):\n",
    "    filehandler = open(filename+'.obj',\"wb\")\n",
    "    pickle.dump(objname,filehandler)\n",
    "\n",
    "# read pickled list of dictionaries for each page in the category: machine learning.\n",
    "def read_pickle_obj(filename):\n",
    "    file = open(filename+'.obj','rb')\n",
    "    object_content = pickle.load(file)\n",
    "    return(object_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pages:  1620 num categories:  62\n"
     ]
    }
   ],
   "source": [
    "print(\"num pages: \", pg_cnt, \"num categories: \", cat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1620/1620 [19:55<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# fetch wikipedia's 'category: machine learning' page content, \n",
    "# for pages in its categories and subcategories.\n",
    "category = \"Category:Machine_learning\"\n",
    "cat_cnt = 1; pg_cnt = 0\n",
    "\n",
    "entire_category_data_list = get_wiki_full_category_content(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'local', 'my_database', 'test', 'wiki_database']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get database names on mongo server.\n",
    "mongoDB_get_DBnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki_ML_collection', 'wiki_ML_clean_collection']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get collection names on specified mongo db.\n",
    "mongoDB_get_collection_names('wiki_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ML collection from mongoDB.\n",
    "ML_raw_collection = mongoDB_read_collection('wiki_database', 'wiki_ML_collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clean text ML collection from mongoDB.\n",
    "ML_clean_collection = mongoDB_read_collection('wiki_database', 'wiki_ML_clean_collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle raw ML pages list.\n",
    "pickle_obj('wiki_ML_rawtext_pages_lst', ML_raw_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle clean ML pages list.\n",
    "pickle_obj('wiki_ML_cleantext_pages_lst', ML_clean_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p><b>Data exploration</b> is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.</p>\\n<p>Data exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.</p>\\n<p>This is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.</p>\\n<p>All of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.</p>\\n<p>Once this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.</p>\\n<p>At this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.</p>\\n<p>Data exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data. In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct.</p>\\n<p>Traditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.</p>\\n<p></p>\\n\\n<p></p>\\n<h2><span id=\"Interactive_Data_Exploration\">Interactive Data Exploration</span></h2>\\n<p>This area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.</p>\\n<p>By employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.</p>\\n<h2><span id=\"Software\">Software</span></h2>\\n<ul><li>Trifacta – a data preparation and analysis platform</li>\\n<li>Paxata – self-service data preparation software</li>\\n<li>Alteryx – data blending and advanced data analytics software</li>\\n<li>IBM Infosphere Analyzer – a data profiling tool</li>\\n<li>Microsoft Power BI - interactive visualization and data analysis tool</li>\\n<li>OpenRefine - a standalone open source desktop application for data clean-up and data transformation</li>\\n<li>Tableau software – interactive data visualization software</li>\\n</ul><h2><span id=\"See_also\">See also</span></h2>\\n\\n<ul><li>Exploratory Data Analysis</li>\\n<li>Machine Learning</li>\\n<li>Data profiling</li>\\n<li>Data Visualization</li>\\n</ul>\\n<h2><span id=\"References\">References</span></h2>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_raw_collection[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.Data exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.This is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using Excel or similar tools to view the raw data.All of these activities are aimed at creating a clear mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.Once this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data, correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.At this stage, the data can be considered ready for deeper analysis or be handed off to other analysts or users who have specific needs for the data.Data exploration can also refer to the adhoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data. In this scenario, hypotheses may be created and then the data is explored to identify whether those hypotheses are correct.Traditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.Interactive Data ExplorationThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As it’s most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Common patterns include regression, classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.By employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.SoftwareTrifacta – a data preparation and analysis platformPaxata – self-service data preparation softwareAlteryx – data blending and advanced data analytics softwareIBM Infosphere Analyzer – a data profiling toolMicrosoft Power BI - interactive visualization and data analysis toolOpenRefine - a standalone open source desktop application for data clean-up and data transformationTableau software – interactive data visualization softwareSee alsoExploratory Data AnalysisMachine LearningData profilingData VisualizationReferences'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_clean_collection[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subcat:  Category:Administrative software depth:  1\n",
      "subcat:  Category:School-administration software depth:  1\n",
      "subcat:  Category:Time-tracking software depth:  2\n",
      "subcat:  Category:Business simulation games depth:  2\n",
      "subcat:  Category:Amusement park simulation games depth:  1\n",
      "subcat:  Category:M.U.L.E. depth:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/564 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXIT subcat:  Category:Roller coaster games and simulations depth:  2\n",
      "EXIT subcat:  Category:Business software companies depth:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [02:15<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num pages:  564 num categories:  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch wikipedia's 'category: Business software' page content, \n",
    "# for pages in its categories and subcategories.\n",
    "# category = \"Category:Business_software\"\n",
    "category = \"Category:Business_software\"\n",
    "\n",
    "cat_cnt = 1; pg_cnt = 0\n",
    "\n",
    "entire_category_data_list = get_wiki_full_category_content(category)\n",
    "\n",
    "print(\"num pages: \", pg_cnt, \"num categories: \", cat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entire_category_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_page_lst = get_wiki_full_category_page_list(category = \"Category:Business_software\", \n",
    "                                               tree_depth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BS_page_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
